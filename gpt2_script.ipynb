{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Libraries used\n",
        "!pip install tiktoken\n",
        "import time\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "## Function that output a tensor of the tokenized text\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "\n",
        "### Multi-Head Attention Class (in the TransformerBlock)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        print(\"Step 1 inside the Multi-Headed Attention Layer: Embedding Matrix.\")\n",
        "        time.sleep(3)\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        print(\"This is the shape of Queries Matrix:\", queries.shape)\n",
        "        print(\"This is the shape of Keys Matrix:\", keys.shape)\n",
        "        print(\"This is the shape of Values Matrix:\", values.shape)\n",
        "        print(\"Example of the Queries Matrix:\", queries)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 2 inside the Multi-Headed Attention Layer: Attention Scores.\")\n",
        "        print(\"We compute the Attention Scores: Queries @ Keys.T\")\n",
        "        time.sleep(3)\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        print(\"This is the shape of the Attention Scores Matrix:\", attn_scores.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(attn_scores)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 3 inside the Multi-Headed Attention Layer: Causal Attention Masking.\")\n",
        "        time.sleep(3)\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "        print(\"This is the output, in which we can see the mask applied:\")\n",
        "        print(attn_scores)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 4 inside the Multi-Headed Attention Layer: Attention Weights.\")\n",
        "        time.sleep(3)\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        print(\"This is the shape of the Attention Weights Matrix:\", attn_weights.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(attn_weights)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        print(\"Step 5 inside the Multi-Headed Attention Layer: Context Vectors.\")\n",
        "        print(\"We compute the Context Vectors: Attention Weights @ Values\")\n",
        "        time.sleep(3)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        print(\"This is the shape of the Context Vectors:\", context_vec.shape)\n",
        "        print(\"And the output itself is:\",)\n",
        "        print(context_vec)\n",
        "        time.sleep(5)\n",
        "\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "\n",
        "### Normalization Class\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-8\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "\n",
        "### GELU Class (Activation Function)\n",
        "class GELU(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "### Feed Forward Class\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"])\n",
        "        self.act_func = GELU()\n",
        "        self.linear2 = nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      print(\"Step 1 inside the Feed Forward Layer: First Linear layer (we expand the embedding dimension).\")\n",
        "      time.sleep(3)\n",
        "      x = self.linear1(x)\n",
        "      print(\"This is the shape of the output of the first Linear Layer:\", x.shape)\n",
        "      time.sleep(2)\n",
        "      print(\"We can see that the embedding dimension has been multiplied by 4.\")\n",
        "      print(\"And the output itself is:\")\n",
        "      print(x)\n",
        "      time.sleep(5)\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "      print(\"Step 2 inside the Feed Forward Layer: Activation Function (non-Linearity).\")\n",
        "      time.sleep(3)\n",
        "      x = self.act_func(x)\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "      print(\"Step 3 inside the Feed Forward Layer: Second Linear layer, we reduce the embedding dimension.\")\n",
        "      time.sleep(3)\n",
        "      x = self.linear2(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "### Transformer Block (in the GPTModel)\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=config[\"emb_dim\"],\n",
        "            d_out=config[\"emb_dim\"],\n",
        "            context_length=config[\"context_length\"],\n",
        "            num_heads=config[\"n_heads\"],\n",
        "            dropout=config[\"drop_rate\"],\n",
        "            qkv_bias=config[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(config)\n",
        "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "\n",
        "        print(\"Step 1 inside the Transformer Block: Normalization layer.\")\n",
        "        time.sleep(3)\n",
        "        x = self.norm1(x)\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 2 inside the Transformer Block: Multi-Head Attention Layer.\")\n",
        "        time.sleep(3)\n",
        "        before_attention = x\n",
        "        x = self.att(x)\n",
        "        print(\"As a reminder, before the multi-headed attention layer, the embedding matrix was as follows:\")\n",
        "        print(before_attention)\n",
        "        time.sleep(5)\n",
        "        print(\"This is the shape of the output of the Multi-Head Attention Layer:\", x.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(x)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 3 inside the Transformer Block: Dropout.\")\n",
        "        time.sleep(3)\n",
        "        x = self.drop_shortcut(x)\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        time.sleep(5)\n",
        "\n",
        "        x = x + shortcut\n",
        "        shortcut = x\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 4 inside the Transformer Block: Normalization layer.\")\n",
        "        time.sleep(3)\n",
        "        x = self.norm2(x)\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 5 inside the Transformer Block: Feed-Forward Layer.\")\n",
        "        time.sleep(3)\n",
        "        x = self.ff(x)\n",
        "        print(\"This is the shape of the output of Feed-Forward Layer:\", x.shape)\n",
        "        print(\"It's the same dimension of the input, despite a dimension change in the layer.\")\n",
        "        print(\"And the output itself is:\")\n",
        "        print(x)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Step 6 inside the Transformer Block: Dropout.\")\n",
        "        time.sleep(3)\n",
        "        x = self.drop_shortcut(x)\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        time.sleep(5)\n",
        "\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "### GPT Model Class\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # (1) Embedding layer\n",
        "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "        # (2) Dropout\n",
        "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "        # (3) Transformer Blocks\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "        # (4) Normalization Layer\n",
        "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "        # (5) Output layer\n",
        "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, in_ids):\n",
        "        batch_size, seq_len = in_ids.shape\n",
        "\n",
        "        print(\"This is the tokenized version of the input text:\")\n",
        "        print(in_ids)\n",
        "        print(\"This is the shape of the input:\", in_ids.shape)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # (1) Embedding Layer\n",
        "        print(\"Step 1 of the GPT Model: Embedding Matrix.\")\n",
        "        time.sleep(3)\n",
        "        tok_embeds = self.tok_emb(in_ids)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_ids.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        start_matrix = x\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(x)\n",
        "        print(\"\\n\")\n",
        "        print(\"We have now an embedding vector for each token.\")\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # (2) Regularization Layer\n",
        "        print(\"Step 2 of the GPT Model: Dropout (regularization).\")\n",
        "        time.sleep(3)\n",
        "        x = self.drop_emb(x)\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(x)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # (3) Transformer Block\n",
        "        print(\"Step 3 of the GPT Model: Transformer Block.\")\n",
        "        time.sleep(3)\n",
        "        x = self.trf_blocks(x)\n",
        "        end_matrix = x\n",
        "        print(\"This is the shape of the final output of the Transformer Block:\", x.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(x)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # (4) Normalization Layer\n",
        "        print(\"Step 4 of the GPT Model: Normalization Layer.\")\n",
        "        time.sleep(3)\n",
        "        x = self.final_norm(x)\n",
        "        print(\"This is the shape of the output of the layer:\", x.shape)\n",
        "        time.sleep(5)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # (5) Output Layer\n",
        "        print(\"Step 5 of the GPT Model (last step): Output Layer.\")\n",
        "        time.sleep(3)\n",
        "        logits = self.out_head(x)\n",
        "        print(\"This is the shape of the final output of the model:\", logits.shape)\n",
        "        print(\"And the output itself is:\")\n",
        "        print(logits)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        return logits, start_matrix, end_matrix\n",
        "\n",
        "\n",
        "\n",
        "### Main Function to run the model\n",
        "def main(txt, tokenizer):\n",
        "  print(\"\\n\")\n",
        "  print(\"The input text is:\", '\"',txt,'\"')\n",
        "  print(\"\\n\")\n",
        "  time.sleep(3)\n",
        "  ## Tokenize the text\n",
        "  txtToken = text_to_token_ids(txt, tokenizer)\n",
        "  ## Create an object instance of the GPTModel\n",
        "  model = GPTModel(basicConfig)\n",
        "  ## Run the model\n",
        "  logits, start_matrix, end_matrix = model(txtToken)\n",
        "\n",
        "\n",
        "\n",
        "### Run the script\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"This script shows the different steps of the model, the input and the output themself have no meaning.\")\n",
        "  print(\"We don't want to train the model, just show the architecture of the model and see how data flows through the different layers. \\n\")\n",
        "  time.sleep(5)\n",
        "\n",
        "  ## Initialize Tokenizer\n",
        "  # we use tiktoken, from Open AI, to tokenize the text\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  ## Basic configurations\n",
        "  basicConfig = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 20,\n",
        "    \"emb_dim\": 12,\n",
        "    \"n_heads\": 1,\n",
        "    \"n_layers\": 1,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "  }\n",
        "\n",
        "  ## Basic input text\n",
        "  txt = \"Explain me how LLM works\"\n",
        "\n",
        "  ## Call the main function\n",
        "  main(txt, tokenizer)"
      ],
      "metadata": {
        "id": "IbHTM2N-xEpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ob_CVkcyEHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}